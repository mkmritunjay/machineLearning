{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KMeans.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMpRPalN3joONTPHsnbR+Jk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkmritunjay/machineLearning/blob/master/KMeans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlEMCca8nOeX",
        "colab_type": "text"
      },
      "source": [
        "<b> Overview </b>\n",
        "- Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). \n",
        "\n",
        "or\n",
        "\n",
        "- Clustering is the task of gathering samples into groups of similar samples according to some predefined similarity or dissimilarity measure (such as the Euclidean distance).\n",
        "\n",
        "\n",
        "\n",
        "Here are some common applications of clustering algorithms:\n",
        "\n",
        "- Primarily used for exploratory data analysis and business applications like customer segmentation, product segmentation, market segmentation.\n",
        "- Compression, in a data reduction sense\n",
        "- Can be used as a preprocessing step for recommender systems\n",
        "- Similarly:\n",
        "   - grouping related web news (e.g. Google News) and web search results\n",
        "   - grouping related stock quotes for investment portfolio management\n",
        "   - building customer profiles for market analysis\n",
        "- Building a code book of prototype samples for unsupervised feature extraction\n",
        "\n",
        "---\n",
        "\n",
        "Using K-means clustering technique\n",
        "- k-mean calculates the distance between the points and the center using euclidean distance and then allocates the points to different clusters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm07imGxnYHI",
        "colab_type": "text"
      },
      "source": [
        "**Clustering comes with assumptions**: A clustering algorithm finds clusters by making assumptions which samples should be grouped together. Each algorithm makes different assumptions and the quality and interpretability of your results will depend on whether the assumptions are satisfied for your goal. \n",
        "\n",
        "> For K-means clustering, the model is that all clusters have equal, spherical variance.\n",
        "\n",
        "**In general, there is no guarantee that structure found by a clustering algorithm has anything to do with what you were interested in**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOvKggxvoNQM",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "# Some Notable Clustering Routines\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png\">\n",
        "\n",
        "<br>\n",
        "---\n",
        "The following are few well-known clustering algorithms. \n",
        "\n",
        "- `sklearn.cluster.KMeans`:  <br>\n",
        "    The simplest, yet effective clustering algorithm. Needs to be provided with the\n",
        "    number of clusters in advance, and assumes that the data is normalized as input\n",
        "    (but use a PCA model as preprocessor). <br>  \n",
        "\n",
        "\n",
        "\n",
        "- `sklearn.cluster.MeanShift`:<br>\n",
        "    Can find better looking clusters than KMeans but is not scalable to high number of samples.<br>  \n",
        "\n",
        "\n",
        "\n",
        "- `sklearn.cluster.DBSCAN`: <br>\n",
        "    Can detect irregularly shaped clusters based on density, i.e. sparse regions in\n",
        "    the input space are likely to become inter-cluster boundaries. Can also detect\n",
        "    outliers (samples that are not part of a cluster).<br>  \n",
        "\n",
        "\n",
        "\n",
        "- `sklearn.cluster.AffinityPropagation`: <br>\n",
        "    Clustering algorithm based on message passing between data points.<br>  \n",
        "\n",
        "\n",
        "\n",
        "- `sklearn.cluster.SpectralClustering`: <br>\n",
        "    KMeans applied to a projection of the normalized graph Laplacian: finds\n",
        "    normalized graph cuts if the affinity matrix is interpreted as an adjacency matrix of a graph.<br>  \n",
        "\n",
        "\n",
        "\n",
        "- `sklearn.cluster.Ward`: <br>\n",
        "    Ward implements hierarchical clustering based on the Ward algorithm,\n",
        "    a variance-minimizing approach. At each step, it minimizes the sum of\n",
        "    squared differences within all clusters (inertia criterion).<br>  \n",
        "    \n",
        "\n",
        "Of these, `Ward, SpectralClustering, DBSCAN` and `Affinity Propagation` can also work with precomputed similarity matrices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al8v9d8Noagb",
        "colab_type": "text"
      },
      "source": [
        "## Part 2: Clustering evaluation (Finding optimal number of clusters)\n",
        "\n",
        "This explains how to find optimal number of clusters in a given dataset by using various techniques.\n",
        "Different techniques discussed here are\n",
        "- Dendogram\n",
        "- Elbow method\n",
        "- Silhoutte score Analysis\n",
        "We will first load the data into dataframe and scale the features and create clusters. And then various metrics are calculated to validate the number of cluster creations and what will be the optimal number of clusters.\n",
        "\n",
        "---\n",
        "The [Silhouette Coefficient](http://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient) is a common metric for evaluating clustering \"performance\" in situations when the \"true\" cluster assignments are not known.\n",
        "\n",
        "A Silhouette Coefficient is calculated for **each observation**:\n",
        "\n",
        "$$SC = \\frac{b-a} {max(a, b)}$$\n",
        "\n",
        "- a = mean distance to all other points in **its cluster**\n",
        "- b = mean distance to all other points in **the next nearest cluster**\n",
        "\n",
        "It ranges from -1 (worst) to 1 (best). A **global score** is calculated by taking the mean score for all observations.\n",
        "\n",
        "---\n",
        "- The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation).\n",
        "- The silhouette ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
        "- If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.\n",
        "- The silhouette can be calculated with any distance metric, such as the Euclidean distance or the Manhattan distance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRaiFpuXokTM",
        "colab_type": "text"
      },
      "source": [
        "## Using Dendogram\n",
        "\n",
        "- Dendogram shows the distance between any two observations in a dataset. The vertical axis determines the distance. The longer the axis, the larger the distance.\n",
        "- The clustermap feature in seaborn provides the dendogram. It also displays the distance between observations as well as the features. But we are mostly interested in observations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xMV8Y9UonQ9",
        "colab_type": "text"
      },
      "source": [
        "# Elbow Analysis\n",
        "\n",
        "- The Elbow method is a method of interpretation and validation of consistency within cluster analysis designed to help finding the appropriate number of clusters in a dataset.\n",
        "- Explained Variance: This method looks at the percentage of variance explained as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data.\n",
        "- if one plots the percentage of variance explained by the clusters against the number of clusters the first clusters will add much information (explain a lot of variance), but at some point the marginal gain in explained variance will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the \"elbow criterion\"\n",
        "- Given a set of observations (x1, x2, …, xn), where each observation is a d-dimensional real vector, k-means clustering aims to partition the n observations into k (≤ n) sets S = {S1, S2, …, Sk} so as to minimize the within-cluster sum of squares (WCSS) (sum of distance functions of each point in the cluster to the K center).\n",
        "-In other words, its objective is to find: argminS∑i=1k∑x∈Si∥x−μi∥2\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe11r7iGnZGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOvTLooJ1fg3EKhTqDghkh1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkmritunjay/machineLearning/blob/master/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqYsGgOgp0lj",
        "colab_type": "text"
      },
      "source": [
        "# Text Mining using Natural Language Processing (NLP)\n",
        "---\n",
        "## Introduction\n",
        "---\n",
        "### What is NLP?\n",
        "\n",
        "- Using computers to process (analyze, understand, generate) natural human languages\n",
        "- Most knowledge created by humans is unstructured text, and we need to find a way to make sense of it\n",
        "- Build probabilistic model using data about a language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7KPa9MwqJep",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Important Packages Related to Textmining\n",
        "- **textmining1.0:** contains a variety of useful functions for text mining in Python.\n",
        "- **NLTK:** This package can be extremely useful because you have easy access to over 50 corpora and lexical resources\n",
        "- **Tweepy:** to mine Twitter data\n",
        "- **scrappy:**  extract the data you need from websites\n",
        "- **urllib2:** a package for opening URLs\n",
        "- **requests:** library for grabbing data from the internet\n",
        "- **Beautifulsoup:** library for parsing HTML data\n",
        "- **re:**  grep(), grepl(), regexpr(), gregexpr(), sub(), gsub(), and strsplit() are helpful functions\n",
        "- **wordcloud:** to visualize the wordcloud\n",
        "- **Textblob:** to used for text processing (nlp- lowel events)\n",
        "- **sklearn:** to used for preprocessing, modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNe93OYLqSYT",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### What are some of the higher level task areas?\n",
        "\n",
        "- **Information retrieval**: Find relevant results and similar results\n",
        "    - [Google](https://www.google.com/)\n",
        "- **Information extraction**: Structured information from unstructured documents\n",
        "    - [Events from Gmail](https://support.google.com/calendar/answer/6084018?hl=en)\n",
        "- **Machine translation**: One language to another\n",
        "    - [Google Translate](https://translate.google.com/)\n",
        "- **Text simplification**: Preserve the meaning of text, but simplify the grammar and vocabulary\n",
        "    - [Rewordify](https://rewordify.com/)\n",
        "    - [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page)\n",
        "- **Predictive text input**: Faster or easier typing\n",
        "    - [My application](https://justmarkham.shinyapps.io/textprediction/)\n",
        "    - [A much better application](https://farsite.shinyapps.io/swiftkey-cap/)\n",
        "- **Sentiment analysis**: Attitude of speaker\n",
        "    - [Hater News](http://haternews.herokuapp.com/)\n",
        "- **Automatic summarization**: Extractive or abstractive summarization\n",
        "    - [autotldr](https://www.reddit.com/r/technology/comments/35brc8/21_million_people_still_use_aol_dialup/cr2zzj0)\n",
        "- **Natural Language Generation**: Generate text from data\n",
        "    - [How a computer describes a sports match](http://www.bbc.com/news/technology-34204052)\n",
        "    - [Publishers withdraw more than 120 gibberish papers](http://www.nature.com/news/publishers-withdraw-more-than-120-gibberish-papers-1.14763)\n",
        "- **Speech recognition and generation**: Speech-to-text, text-to-speech\n",
        "    - [Google's Web Speech API demo](https://www.google.com/intl/en/chrome/demos/speech.html)\n",
        "    - [Vocalware Text-to-Speech demo](https://www.vocalware.com/index/demo)\n",
        "- **Question answering**: Determine the intent of the question, match query with knowledge base, evaluate hypotheses\n",
        "    - [How did supercomputer Watson beat Jeopardy champion Ken Jennings?](http://blog.ted.com/how-did-supercomputer-watson-beat-jeopardy-champion-ken-jennings-experts-discuss/)\n",
        "    - [IBM's Watson Trivia Challenge](http://www.nytimes.com/interactive/2010/06/16/magazine/watson-trivia-game.html)\n",
        "    - [The AI Behind Watson](http://www.aaai.org/Magazine/Watson/watson.php)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMZvLXiDqZU1",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Data Processing - What are some of the lower level components?\n",
        "\n",
        "- **Tokenization**: breaking text into tokens (words, sentences, n-grams)\n",
        "- **Stopword removal**: a/an/the\n",
        "- **Stemming and lemmatization**: root word\n",
        "- **TF-IDF**: word importance\n",
        "- **Part-of-speech tagging**: noun/verb/adjective\n",
        "- **Named entity recognition**: person/organization/location\n",
        "- **Spelling correction**: \"New Yrok City\"\n",
        "- **Word sense disambiguation**: \"buy a mouse\"\n",
        "- **Segmentation**: \"New York City subway\"\n",
        "- **Language detection**: \"translate this page\"\n",
        "- **Machine learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOX5q2k6qgqP",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Why NLP is hard?\n",
        "\n",
        "- **Ambiguity**:\n",
        "    - Hospitals are Sued by 7 Foot Doctors\n",
        "    - Juvenile Court to Try Shooting Defendant\n",
        "    - Local High School Dropouts Cut in Half\n",
        "- **Non-standard English**: text messages\n",
        "- **Idioms**: \"throw in the towel\"\n",
        "- **Newly coined words**: \"retweet\"\n",
        "- **Tricky entity names**: \"Where is A Bug's Life playing?\"\n",
        "- **World knowledge**: \"Mary and Sue are sisters\", \"Mary and Sue are mothers\"\n",
        "\n",
        "NLP requires an understanding of the **language** and the **world**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoRFhrGrswhP",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Text Classification\n",
        "\n",
        "#### Feature Engineering\n",
        "##### TF-IDF Vectors as features\n",
        "- TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
        "- IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
        "\n",
        "- TF-IDF Vectors can be generated at different levels of input tokens (words, characters, n-grams)\n",
        "    - a. Word Level TF-IDF : Matrix representing tf-idf scores of every term in different documents\n",
        "    - b. N-gram Level TF-IDF : N-grams are the combination of N terms together. This Matrix representing tf-idf scores of N-grams\n",
        "    - c. Character Level TF-IDF : Matrix representing tf-idf scores of character level n-grams in the corpus\n",
        "\n",
        "##### Text / NLP based features\n",
        "- Word Count of the documents – total number of words in the documents\n",
        "- Character Count of the documents – total number of characters in the documents\n",
        "- Average Word Density of the documents – average length of the words used in the documents\n",
        "- Puncutation Count in the Complete Essay – total number of punctuation marks in the documents\n",
        "- Upper Case Count in the Complete Essay – total number of upper count words in the documents\n",
        "- Title Word Count in the Complete Essay – total number of proper case (title) words in the documents\n",
        "- Frequency distribution of Part of Speech Tags:\n",
        "    - Noun Count\n",
        "    - Verb Count\n",
        "    - Adjective Count\n",
        "    - Adverb Count\n",
        "    - ronoun Count\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC7tNi2ns7vR",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Model Building\n",
        "- Naive Bayes Classifier\n",
        "- Linear Classifier\n",
        "- Support Vector Machine\n",
        "- KNN\n",
        "- Bagging Models\n",
        "- Boosting Models\n",
        "- Shallow Neural Networks\n",
        "- Deep Neural Networks\n",
        "    - Convolutional Neural Network (CNN)\n",
        "    - Long Short Term Modelr (LSTM)\n",
        "    - Gated Recurrent Unit (GRU)\n",
        "    - Bidirectional RNN\n",
        "    - Recurrent Convolutional Neural Network (RCNN)\n",
        "    - Other Variants of Deep Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI8K_Ep5tGff",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Part 1: Reading in the Yelp Reviews\n",
        "\n",
        "- \"corpus\" = collection of documents\n",
        "- \"corpora\" = plural form of corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WOPN2fps9pt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "badee8e9-37aa-4581-b832-aa7673690b0b"
      },
      "source": [
        "#import required packages\n",
        "#basics\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "\n",
        "#misc\n",
        "import gc\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "#stats\n",
        "#from scipy.misc import imread\n",
        "from scipy import sparse\n",
        "import scipy.stats as ss\n",
        "\n",
        "#viz\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec \n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud ,STOPWORDS\n",
        "from PIL import Image\n",
        "#import matplotlib_venn as venn\n",
        "\n",
        "#nlp\n",
        "import string\n",
        "import re    #for regex\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#import spacy\n",
        "from nltk import pos_tag\n",
        "from nltk.stem.wordnet import WordNetLemmatizer \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tweet tokenizer does not split at apostophes which is what we want\n",
        "from nltk.tokenize import TweetTokenizer   \n",
        "\n",
        "\n",
        "#FeatureEngineering\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer, TfidfTransformer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, decomposition, ensemble\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import  textblob\n",
        "#import xgboost\n",
        "#from keras.preprocessing import text, sequence\n",
        "#from keras import layers, models, optimizers\n",
        "\n",
        "from textblob import TextBlob\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "#nltk.download('wordnet')\n",
        "from textblob import Word\n",
        "\n",
        "#settings\n",
        "start_time=time.time()\n",
        "color = sns.color_palette()\n",
        "sns.set_style(\"dark\")\n",
        "eng_stopwords = set(stopwords.words(\"english\"))\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "lem = WordNetLemmatizer()\n",
        "tokenizer=TweetTokenizer()\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKUA5gpHtXki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/mkmritunjay/machineLearning/master/yelp.csv'\n",
        "yelp = pd.read_csv(url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1nVa_oauYC2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2f46d6fa-6457-4ca1-f128-f965323dff7f"
      },
      "source": [
        "yelp.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>date</th>\n",
              "      <th>review_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>text</th>\n",
              "      <th>type</th>\n",
              "      <th>user_id</th>\n",
              "      <th>cool</th>\n",
              "      <th>useful</th>\n",
              "      <th>funny</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
              "      <td>2011-01-26</td>\n",
              "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
              "      <td>5</td>\n",
              "      <td>My wife took me here on my birthday for breakf...</td>\n",
              "      <td>review</td>\n",
              "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
              "      <td>2011-07-27</td>\n",
              "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
              "      <td>5</td>\n",
              "      <td>I have no idea why some people give bad review...</td>\n",
              "      <td>review</td>\n",
              "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
              "      <td>2012-06-14</td>\n",
              "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
              "      <td>4</td>\n",
              "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
              "      <td>review</td>\n",
              "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
              "      <td>2010-05-27</td>\n",
              "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
              "      <td>5</td>\n",
              "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
              "      <td>review</td>\n",
              "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
              "      <td>2012-01-05</td>\n",
              "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
              "      <td>5</td>\n",
              "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
              "      <td>review</td>\n",
              "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              business_id        date  ... useful  funny\n",
              "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  ...      5      0\n",
              "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  ...      0      0\n",
              "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  ...      1      0\n",
              "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  ...      2      0\n",
              "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  ...      0      0\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zqqRITLuaTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove some unwanted variables\n",
        "yelp=yelp[['review_id', 'stars', 'text', 'cool', 'useful', 'funny']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psTn5ebxugfX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "af0cdc2b-eba5-4f20-8aa0-688cc0ad367b"
      },
      "source": [
        "yelp.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>text</th>\n",
              "      <th>cool</th>\n",
              "      <th>useful</th>\n",
              "      <th>funny</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
              "      <td>5</td>\n",
              "      <td>My wife took me here on my birthday for breakf...</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
              "      <td>5</td>\n",
              "      <td>I have no idea why some people give bad review...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
              "      <td>4</td>\n",
              "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
              "      <td>5</td>\n",
              "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
              "      <td>5</td>\n",
              "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                review_id  stars  ... useful  funny\n",
              "0  fWKvX83p0-ka4JS3dc6E5A      5  ...      5      0\n",
              "1  IjZ33sJrzXqU-0X6U8NwyA      5  ...      0      0\n",
              "2  IESLBzqUCLdSzSqm0eCSxQ      4  ...      1      0\n",
              "3  G-WvGaISbqqaMHlNnByodA      5  ...      2      0\n",
              "4  1uJFq2r5QfJG_6ExMRCaGw      5  ...      0      0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSp1wHYZuiDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = yelp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y_1Aee7utsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data exploration\n",
        "df['text'] = df['text'].astype(str)\n",
        "df['count_sent']=df[\"text\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n",
        "\n",
        "#Word count in each comment:\n",
        "df['count_word']=df[\"text\"].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "#Unique word count\n",
        "df['count_unique_word']=df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
        "\n",
        "#Letter count\n",
        "df['count_letters']=df[\"text\"].apply(lambda x: len(str(x)))\n",
        "\n",
        "#Word density\n",
        "\n",
        "df['word_density'] = df['count_letters'] / (df['count_word']+1)\n",
        "\n",
        "#punctuation count\n",
        "df[\"count_punctuations\"] =df[\"text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
        "\n",
        "#upper case words count\n",
        "df[\"count_words_upper\"] = df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
        "\n",
        "#upper case words count\n",
        "df[\"count_words_lower\"] = df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.islower()]))\n",
        "\n",
        "#title case words count\n",
        "df[\"count_words_title\"] = df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
        "\n",
        "#Number of stopwords\n",
        "df[\"count_stopwords\"] = df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
        "\n",
        "#Average length of the words\n",
        "df[\"mean_word_len\"] = df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "\n",
        "#Number of numeric\n",
        "df['numeric'] = df['text'].apply(lambda x :len([x for x in x.split() if x.isdigit()]))\n",
        "\n",
        "#Number of alphanumeric\n",
        "df['alphanumeric'] = df['text'].apply(lambda x :len([x for x in x.split() if x.isalnum()]))\n",
        "\n",
        "#Number of alphabetics\n",
        "df['alphabetetics'] = df['text'].apply(lambda x :len([x for x in x.split() if x.isalpha()]))\n",
        "\n",
        "#Number of alphabetics\n",
        "df['Spaces'] = df['text'].apply(lambda x :len([x for x in x.split() if x.isspace()]))\n",
        "\n",
        "#Number of Words ends with\n",
        "df['words_ends_with_et'] = df['text'].apply(lambda x :len([x for x in x.lower().split() if x.endswith('et')]))\n",
        "\n",
        "#Number of Words ends with\n",
        "df['words_start_with_no'] = df['text'].apply(lambda x :len([x for x in x.lower().split() if x.startswith('no')]))\n",
        "\n",
        "# Count the occurences of all words\n",
        "df['wordcounts'] = df['text'].apply(lambda x :dict([ [t, x.split().count(t)] for t in set(x.split()) ]))\n",
        "\n",
        "pos_family = {\n",
        "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
        "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
        "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "    'adj' :  ['JJ','JJR','JJS'],\n",
        "    'adv' : ['RB','RBR','RBS','WRB']\n",
        "}\n",
        "\n",
        "# function to check and get the part of speech tag count of a words in a given sentence\n",
        "def check_pos_tag(x, flag):\n",
        "    cnt = 0\n",
        "    try:\n",
        "        wiki = textblob.TextBlob(x)\n",
        "        for tup in wiki.tags:\n",
        "            ppo = list(tup)[1]\n",
        "            if ppo in pos_family[flag]:\n",
        "                cnt += 1\n",
        "    except:\n",
        "        pass\n",
        "    return cnt\n",
        "\n",
        "df['noun_count'] = df['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
        "df['verb_count'] = df['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
        "df['adj_count']  = df['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
        "df['adv_count']  = df['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
        "df['pron_count'] = df['text'].apply(lambda x: check_pos_tag(x, 'pron')) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib10mLdCu2XZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['sentiment'] = df[\"text\"].apply(lambda x: TextBlob(x).sentiment.polarity )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIqalyJN0DME",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "79f12830-7e37-4b05-bc57-ceedec836a04"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>text</th>\n",
              "      <th>cool</th>\n",
              "      <th>useful</th>\n",
              "      <th>funny</th>\n",
              "      <th>count_sent</th>\n",
              "      <th>count_word</th>\n",
              "      <th>count_unique_word</th>\n",
              "      <th>count_letters</th>\n",
              "      <th>word_density</th>\n",
              "      <th>count_punctuations</th>\n",
              "      <th>count_words_upper</th>\n",
              "      <th>count_words_lower</th>\n",
              "      <th>count_words_title</th>\n",
              "      <th>count_stopwords</th>\n",
              "      <th>mean_word_len</th>\n",
              "      <th>numeric</th>\n",
              "      <th>alphanumeric</th>\n",
              "      <th>alphabetetics</th>\n",
              "      <th>Spaces</th>\n",
              "      <th>words_ends_with_et</th>\n",
              "      <th>words_start_with_no</th>\n",
              "      <th>wordcounts</th>\n",
              "      <th>noun_count</th>\n",
              "      <th>verb_count</th>\n",
              "      <th>adj_count</th>\n",
              "      <th>adv_count</th>\n",
              "      <th>pron_count</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
              "      <td>5</td>\n",
              "      <td>My wife took me here on my birthday for breakf...</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>155</td>\n",
              "      <td>110</td>\n",
              "      <td>889</td>\n",
              "      <td>5.698718</td>\n",
              "      <td>21</td>\n",
              "      <td>3</td>\n",
              "      <td>134</td>\n",
              "      <td>16</td>\n",
              "      <td>71</td>\n",
              "      <td>4.670968</td>\n",
              "      <td>1</td>\n",
              "      <td>135</td>\n",
              "      <td>134</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>{'Our': 1, 'blend': 1, 'absolutely': 1, 'excel...</td>\n",
              "      <td>26</td>\n",
              "      <td>35</td>\n",
              "      <td>19</td>\n",
              "      <td>14</td>\n",
              "      <td>28</td>\n",
              "      <td>0.402469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
              "      <td>5</td>\n",
              "      <td>I have no idea why some people give bad review...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>257</td>\n",
              "      <td>159</td>\n",
              "      <td>1345</td>\n",
              "      <td>5.213178</td>\n",
              "      <td>36</td>\n",
              "      <td>6</td>\n",
              "      <td>225</td>\n",
              "      <td>25</td>\n",
              "      <td>134</td>\n",
              "      <td>4.225681</td>\n",
              "      <td>0</td>\n",
              "      <td>227</td>\n",
              "      <td>227</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>{'placed': 1, 'can': 2, 'That': 1, 'thought': ...</td>\n",
              "      <td>48</td>\n",
              "      <td>53</td>\n",
              "      <td>19</td>\n",
              "      <td>18</td>\n",
              "      <td>35</td>\n",
              "      <td>0.229773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
              "      <td>4</td>\n",
              "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>76</td>\n",
              "      <td>4.470588</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>3.812500</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>{'I': 1, 'selection': 1, 'plate.': 1, 'dig': 1...</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.566667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
              "      <td>5</td>\n",
              "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>76</td>\n",
              "      <td>61</td>\n",
              "      <td>419</td>\n",
              "      <td>5.441558</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>61</td>\n",
              "      <td>13</td>\n",
              "      <td>33</td>\n",
              "      <td>4.486842</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>{'can': 1, 'You': 1, 'lot': 1, 'cans': 1, 'LOV...</td>\n",
              "      <td>28</td>\n",
              "      <td>12</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.608646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
              "      <td>5</td>\n",
              "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>86</td>\n",
              "      <td>72</td>\n",
              "      <td>469</td>\n",
              "      <td>5.390805</td>\n",
              "      <td>38</td>\n",
              "      <td>2</td>\n",
              "      <td>70</td>\n",
              "      <td>12</td>\n",
              "      <td>44</td>\n",
              "      <td>4.453488</td>\n",
              "      <td>0</td>\n",
              "      <td>69</td>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>{'say.....': 1, 'walk': 1, ':^)': 1, 'Petello'...</td>\n",
              "      <td>21</td>\n",
              "      <td>19</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>14</td>\n",
              "      <td>0.468125</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                review_id  stars  ... pron_count  sentiment\n",
              "0  fWKvX83p0-ka4JS3dc6E5A      5  ...         28   0.402469\n",
              "1  IjZ33sJrzXqU-0X6U8NwyA      5  ...         35   0.229773\n",
              "2  IESLBzqUCLdSzSqm0eCSxQ      4  ...          2   0.566667\n",
              "3  G-WvGaISbqqaMHlNnByodA      5  ...          3   0.608646\n",
              "4  1uJFq2r5QfJG_6ExMRCaGw      5  ...         14   0.468125\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSqyXPXy0Kaj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9efb9ffd-4e77-42bc-fcd0-a8060cc49fd8"
      },
      "source": [
        "yelp.stars.value_counts()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    3526\n",
              "5    3337\n",
              "3    1461\n",
              "2     927\n",
              "1     749\n",
              "Name: stars, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLkwPn4L0Z6X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3714a15c-7959-4b95-dba0-7cf22a78d031"
      },
      "source": [
        "# define X and y\n",
        "X = yelp.text\n",
        "y = yelp.stars\n",
        "\n",
        "# split the new DataFrame into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7500,)\n",
            "(2500,)\n",
            "(7500,)\n",
            "(2500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TknkpWo00iGr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Abbrevations and Words correction\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.strip()\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:{}`+=~|.!?,'0-9]\", \"\", text)\n",
        "    return(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2RwtMWc0wi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "def pre_process(text):\n",
        "    #text = text.str.replace('/','')\n",
        "    #text = text.apply(lambda x: re.sub(\"  \",\" \", x))\n",
        "    #text = re.sub(r\"[-()\\\"#/@;:{}`+=~|.!?,']\", \"\", text)\n",
        "    #text = re.sub(r'[0-9]+', '', text)\n",
        "    #text = text.apply(lambda x: \" \".join(x.translate(str.maketrans('', '', string.punctuation)) for x in x.split() if x.isalpha()))\n",
        "    text = text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "    #text = text.apply(lambda x: str(TextBlob(x).correct()))\n",
        "    #text = text.apply(lambda x: \" \".join(PorterStemmer().stem(word) for word in x.split()))\n",
        "    #text = text.apply(lambda x: \" \".join(stemmer_func(word) for word in x.split()))\n",
        "    #text = text.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "    #text = text.apply(lambda x: \" \".join(word for word, pos in pos_tag(x.split()) if pos not in ['NN','NNS','NNP','NNPS']))\n",
        "    return(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpQGBtWj07MH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.apply(lambda x: clean_text(x))\n",
        "X_test = X_test.apply(lambda x: clean_text(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwACcW1s1BBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_train=pre_process(X_train)\n",
        "#X_test=pre_process(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFuwEueU19C1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
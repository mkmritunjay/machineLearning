{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANNClassifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNeoJjf6XS1NfB1VMRHpr5p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkmritunjay/machineLearning/blob/master/ANNClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK-wmTVKoSaU",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks\n",
        " \n",
        "Neural Networks are a machine learning framework that attempts to mimic the learning pattern of natural biological neural networks. Biological neural networks have interconnected neurons with dendrites that receive inputs, then based on these inputs they produce an output signal through an axon to another neuron. We will try to mimic this process through the use of Artificial Neural Networks (ANN), which we will just refer to as neural networks from now on. The process of creating a neural network begins with the most basic form, a single perceptron.\n",
        "\n",
        "---\n",
        "The Perceptron\n",
        " \n",
        "Let's start our discussion by talking about the Perceptron! A perceptron has one or more inputs, a bias, an activation function, and a single output. The perceptron receives inputs, multiplies them by some weight, and then passes them into an activation function to produce an output. There are many possible activation functions to choose from, such as the logistic function, a trigonometric function, a step function etc. We also make sure to add a bias to the perceptron, this avoids issues where all inputs could be equal to zero (meaning no multiplicative weight would have an effect).\n",
        "\n",
        "\n",
        "---\n",
        "Once we have the output we can compare it to a known label and adjust the weights accordingly (the weights usually start off with random initialization values). We keep repeating this process until we have reached a maximum number of allowed iterations, or an acceptable error rate.\n",
        "\n",
        "To create a neural network, we simply begin to add layers of perceptrons together, creating a multi-layer perceptron model of a neural network. You'll have an input layer which directly takes in your feature inputs and an output layer which will create the resulting outputs. Any layers in between are known as hidden layers because they don't directly \"see\" the feature inputs or outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBUse5bco5Lg",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "Data\n",
        " \n",
        "We'll use SciKit Learn's built in Breast Cancer Data Set which has several features of tumors with a labeled class indicating whether the tumor was Malignant or Benign. We will try to create a neural network model that can take in these features and attempt to predict malignant or benign labels for tumors it has not seen before. Let's go ahead and start by getting the data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09St20alp84B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report,confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSTE3mymoLee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This object is like a dictionary, it contains a description of the data and the features and targets:\n",
        "cancer = load_breast_cancer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV4R8i5HpJHW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c79ad2ce-dcea-4930-879e-e263f873952e"
      },
      "source": [
        "cancer.keys()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HLZNT4LpOmG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1201624-e22d-48ec-b71a-7a0a154d1a15"
      },
      "source": [
        "cancer['data'].shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz7S6C09pTvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = cancer['data']\n",
        "Y = cancer['target']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okOZLQZRpfTQ",
        "colab_type": "text"
      },
      "source": [
        "### Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95ooBUZApecY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X12PkI_cqXFa",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocessing\n",
        " \n",
        "The neural network may have difficulty converging before the maximum number of iterations allowed if the data is not normalized. Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data. Note that you must apply the same scaling to the test set for meaningful results. There are a lot of different methods for normalization of data, we will use the built-in StandardScaler for standardization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w-_8eREqP_u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6bba499-551f-4499-99ae-bd9bdff37b1a"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "# Fit only to the training data\n",
        "scaler.fit(X_train)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StandardScaler(copy=True, with_mean=True, with_std=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2O1wnfOqj5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now apply the transformations to the data:\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yjov3FagrSaX",
        "colab_type": "text"
      },
      "source": [
        "### Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vilxU6VOqxMe",
        "colab_type": "text"
      },
      "source": [
        "Now it's time to train our model. SciKit Learn makes this incredibly easy, by using estimator objects. In this case we will import our estimator (the Multi-Layer Perceptron Classifier model) from the neural_network library of SciKit-Learn!\n",
        "\n",
        "**from sklearn.neural_network import MLPClassifier**\n",
        "\n",
        "Next we create an instance of the model, there are a lot of parameters you can choose to define and customize here, we will only define the hidden_layer_sizes. For this parameter you pass in a tuple consisting of the number of neurons you want at each layer, where the nth entry in the tuple represents the number of neurons in the nth layer of the MLP model. There are many ways to choose these numbers, but for simplicity we will choose 3 layers with the same number of neurons as there are features in our data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bVq_vPjqm8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(30,30,30))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMNkr-G-rJqY",
        "colab_type": "text"
      },
      "source": [
        "Now that the model has been made we can fit the training data to our model, remember that this data has already been processed and scaled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKYTiAlerG5o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "d0b80ba3-7db6-4322-bb6c-2a88d0ad04fd"
      },
      "source": [
        "mlp.fit(X_train,y_train)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(30, 30, 30), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOA0XgrIrZ5w",
        "colab_type": "text"
      },
      "source": [
        "### Prediction and evaluation\n",
        "\n",
        "Now that we have a model it's time to use it to get predictions. \n",
        "\n",
        "We can do this simply with the predict() method off of our fitted model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgzYdWc3rVvL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = mlp.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeZGuFuArsEw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "dafac788-497d-4987-f28d-84f41f8218a9"
      },
      "source": [
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97        53\n",
            "           1       0.98      0.99      0.98        90\n",
            "\n",
            "    accuracy                           0.98       143\n",
            "   macro avg       0.98      0.98      0.98       143\n",
            "weighted avg       0.98      0.98      0.98       143\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbYW-DpBsAse",
        "colab_type": "text"
      },
      "source": [
        "With a 98% accuracy rate (as well as 98% precision and recall) this is pretty good considering how few lines of code we had to write. The downside however to using a Multi-Layer Preceptron model is how difficult it is to interpret the model itself. The weights and biases won't be easily interpretable in relation to which features are important to the model itself.\n",
        "\n",
        "However, if you do want to extract the MLP weights and biases after training your model, you use its public attributes coefs_ and intercepts_.\n",
        "\n",
        "coefs_ is a list of weight matrices, where weight matrix at index i represents the weights between layer i and layer i+1.\n",
        "\n",
        "intercepts_ is a list of bias vectors, where the vector at index i represents the bias values added to layer i+1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfU3fkS_rulJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "703e0502-55ff-4da4-fccf-34bf6fc908a0"
      },
      "source": [
        "len(mlp.coefs_)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzLvZMTpsaDe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a65ccd8-bc84-47b2-84eb-8caecb539ee6"
      },
      "source": [
        "len(mlp.coefs_[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7ndMYtqscJ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "381d6717-c5d3-48dc-fdf4-dceca13f93bc"
      },
      "source": [
        "len(mlp.intercepts_[0])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS7YEg9jsf4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}